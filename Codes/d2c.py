# -*- coding: utf-8 -*-
"""D2C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UzKKaRN_c0AgMfa0_CrY_1HlLBFePEA-
"""

!pip install tigramite

import numpy as np
import pandas as pd
from scipy import stats
from tigramite import plotting as tp
import matplotlib.pyplot as plt
from pathlib import Path

def get_nonlinear_equations_with_confounder(n_vars, max_lag):
    """Get nonlinear equations with confounder U (max 2 edges from U)"""
    if n_vars == 4:
        if max_lag == 2:
            return [
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 3:
            return [
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 4:
            return [
                "X4[t] = 0.25 * X1[t-4]^2 - 0.1 * X1[t-4]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
    elif n_vars == 6:
        if max_lag == 2:
            return [
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 3:
            return [
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 4:
            return [
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-4]^2 - 0.1 * X1[t-4]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
    elif n_vars == 8:
        if max_lag == 2:
            return [
                "X8[t] = 0.4 * X7[t]^2 - 0.12 * X7[t]^3 + e8",
                "X7[t] = 0.35 * X6[t-1]^2 - 0.08 * X6[t-1]^3 + e7",
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 3:
            return [
                "X8[t] = 0.4 * X7[t]^2 - 0.12 * X7[t]^3 + e8",
                "X7[t] = 0.35 * X6[t-1]^2 - 0.08 * X6[t-1]^3 + e7",
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-2]^2 - 0.1 * X1[t-2]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
        elif max_lag == 4:
            return [
                "X8[t] = 0.4 * X7[t]^2 - 0.12 * X7[t]^3 + e8",
                "X7[t] = 0.35 * X6[t-1]^2 - 0.08 * X6[t-1]^3 + e7",
                "X6[t] = 0.85 * X5[t]^2 - 0.15 * X5[t]^3 + e6",
                "X5[t] = 0.3 * X4[t-1]^2 - 0.1 * X4[t-1]^3 + e5",
                "X4[t] = 0.25 * X1[t-4]^2 - 0.1 * X1[t-4]^3 + e4",
                "X3[t] = 0.35 * X4[t]^2 - 0.15 * X4[t]^3 + 0.2 * X2[t-3]^2 + 0.3 * U[t]^2 + e3",
                "X2[t] = 0.3 * X3[t-1]^2 - 0.05 * X3[t-1]^3 + e2",
                "X1[t] = 0.4 * X2[t]^2 - 0.2 * X2[t]^3 + 0.5 * U[t]^2 + e1",
                "U[t] = eU"
            ]
    return []

class BlockMissingNonlinearConfoundedGenerator:
    def __init__(self, block_size_range=(5, 20), block_prob=0.2, noise_type='gaussian',
                 noise_params={'scale': 0.1, 'df': 3}, random_state=None):
        """
        Initialize generator with block missingness pattern and confounder

        Parameters:
        block_size_range: tuple, (min_size, max_size) for missing blocks
        block_prob: float between 0 and 1, probability of starting a missing block
        noise_type: str, 'gaussian' or 'student_t'
        noise_params: dict with 'scale' and 'df' (for Student's t) parameters
        random_state: int, random seed
        """
        self.block_size_range = block_size_range
        self.block_prob = block_prob
        self.noise_type = noise_type
        self.noise_params = noise_params
        self.random_state = random_state
        if random_state is not None:
            np.random.seed(random_state)

    def generate_noise(self, size):
        """Generate noise based on specified distribution"""
        if self.noise_type == 'gaussian':
            return np.random.normal(0, self.noise_params['scale'], size=size)
        elif self.noise_type == 'student_t':
            return stats.t.rvs(df=self.noise_params['df'],
                             loc=0,
                             scale=self.noise_params['scale'],
                             size=size)

    def generate_irregular_timestamps(self, n_points, total_time, min_gap=0.1):
        """Generate irregular sampling times"""
        times = np.zeros(n_points)
        times[0] = np.random.uniform(0, min_gap)

        for i in range(1, n_points):
            gap = np.random.exponential(scale=(total_time-times[i-1])/(n_points-i))
            times[i] = times[i-1] + max(gap, min_gap)

            if times[i] > total_time:
                times = times * (total_time / times[i])

        return times

    def find_nearest_lag_idx(self, timestamps, current_idx, lag_time):
        """Find index of nearest available past observation for given lag"""
        target_time = timestamps[current_idx] - lag_time
        past_timestamps = timestamps[:current_idx]
        if len(past_timestamps) == 0:
            return 0
        return (np.abs(past_timestamps - target_time)).argmin()

    def generate_block_missing_pattern(self, n_points, n_vars):
        """Generate block missingness pattern (MAR/NMAR) - U is never missing"""
        missing_mask = np.zeros((n_points, n_vars), dtype=bool)
        current_block = False
        block_remaining = 0

        for t in range(n_points):
            if not current_block and np.random.random() < self.block_prob:
                # Start new missing block
                current_block = True
                block_remaining = np.random.randint(*self.block_size_range)

                # Choose which variables to affect (MAR - depends on other variables)
                n_affected = np.random.randint(1, n_vars + 1)
                affected_vars = np.random.choice(n_vars, size=n_affected, replace=False)

                # Set missing pattern for this block
                for _ in range(block_remaining):
                    if t + _ < n_points:
                        missing_mask[t + _, affected_vars] = True

            if current_block:
                block_remaining -= 1
                if block_remaining <= 0:
                    current_block = False

        return missing_mask

    def generate_equations(self, t, X, U, lag_indices, n_vars, max_lag):
        """Execute nonlinear equations with confounder"""
        noise = self.generate_noise(n_vars + 1)  # +1 for U
        equations = get_nonlinear_equations_with_confounder(n_vars, max_lag)
        true_links = extract_coefficients_from_equations(equations)

        # Generate U first (confounder)
        U[t] = noise[-1]

        # Generate values based on nonlinear relationships
        for i in range(n_vars-1, -1, -1):
            var_name = f'X{i+1}'
            value = 0

            # Add causal influences with polynomial terms
            for (source, lag, target, power), coef in true_links.items():
                if target == var_name:
                    if source == 'U':
                        value += coef * (U[t] ** power)
                    else:
                        source_idx = int(source[1:]) - 1
                        if lag == 0:
                            value += coef * (X[t, source_idx] ** power)
                        else:
                            lag_idx = lag_indices[abs(lag)-1]
                            value += coef * (X[lag_idx, source_idx] ** power)

            # Add noise term
            X[t, i] = value + noise[i]

    def generate_multivariate_ts(self, n_points, n_vars, max_lag, total_time=100, min_gap=0.1):
        """Generate multivariate time series with block missingness and confounder"""
        # Initialize arrays
        X = np.zeros((n_points, n_vars))
        U = np.zeros(n_points)  # Array for confounder U

        # Generate irregular timestamps
        timestamps = self.generate_irregular_timestamps(n_points, total_time, min_gap)

        # Initialize first steps with noise
        for i in range(max_lag):
            X[i] = self.generate_noise(n_vars)
            U[i] = self.generate_noise(1)[0]

        # Generate time series
        for t in range(max_lag, n_points):
            mean_diff = np.mean(np.diff(timestamps))
            lag_indices = [self.find_nearest_lag_idx(timestamps, t, i * mean_diff)
                         for i in range(1, max_lag + 1)]

            self.generate_equations(t, X, U, lag_indices, n_vars, max_lag)

        # Generate and apply block missingness pattern (only to observed variables)
        missing_mask = self.generate_block_missing_pattern(n_points, n_vars)
        X_missing = X.copy()
        X_missing[missing_mask] = np.nan

        # Create DataFrames for both complete and missing data
        columns = [f'X{i+1}' for i in range(n_vars)]
        df_missing = pd.DataFrame(X_missing, columns=columns)
        df_missing['U'] = U  # U is always fully observed
        df_missing['time'] = timestamps

        df_complete = pd.DataFrame(X, columns=columns)
        df_complete['U'] = U
        df_complete['time'] = timestamps

        return df_missing, df_complete, missing_mask

def extract_coefficients_from_equations(equations):
    """Extract coefficients and relationships from equations including U"""
    causal_links = {}

    for eq in equations:
        if '=' not in eq:
            continue

        left, right = [side.strip() for side in eq.split('=')]
        if 'e' in right and len(right.split('+')) == 1:
            continue  # Skip pure noise equations

        target = left.split('[')[0]
        terms = [term.strip() for term in right.split('+')]

        for term in terms:
            if '*' not in term or not ('X' in term or 'U' in term):
                continue

            parts = term.split('*')
            coeff = float(parts[0].strip())
            var_part = parts[1].strip()
            base_var = var_part.split('^')[0] if '^' in var_part else var_part
            var = base_var.split('[')[0]

            power = 1
            if '^' in var_part:
                power = int(var_part.split('^')[1].split(' ')[0])

            if var == 'U':
                lag = 0
            else:
                lag_part = base_var.split('[')[1].split(']')[0]
                lag = 0 if lag_part == 't' else -int(lag_part.split('-')[1])

            causal_links[(var, lag, target, power)] = coeff

    return causal_links

def extract_linear_links_for_graph(equations):
    """Extract base linear coefficients for graph visualization"""
    links = {}

    for eq in equations:
        if '=' in eq:
            left, right = [side.strip() for side in eq.split('=')]
            target = left.split('[')[0]

            if target != 'U':  # Skip U's equation
                terms = [term.strip() for term in right.split('+')]
                for term in terms:
                    if '*' in term and ('X' in term or 'U' in term):
                        # Get base coefficient
                        parts = term.split('*')
                        coeff = float(parts[0].strip())
                        var_part = parts[1].strip()
                        var = var_part.split('^')[0].split('[')[0]

                        if var == 'U':
                            lag = 0
                        else:
                            lag_part = var_part.split('[')[1].split(']')[0]
                            lag = 0 if lag_part == 't' else -int(lag_part.split('-')[1])

                        links[(var, lag, target)] = coeff

    return links

def save_dataset_and_visualizations(df_missing, df_complete, missing_mask, n_vars, max_lag,
                                  sample_size, noise_type, block_prob, output_dir="output_block_missing_confounded"):
    """Save dataset and create visualizations including causal graph"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    base_filename = f'{output_dir}/nonlinear_ts_n{sample_size}_vars{n_vars}_lag{max_lag}_{noise_type}_block{int(block_prob*100)}'

    # Save datasets
    df_missing.to_csv(f'{base_filename}_missing.csv', index=False)
    df_complete.to_csv(f'{base_filename}_complete.csv', index=False)

    # Create and save causal graph
    equations = get_nonlinear_equations_with_confounder(n_vars, max_lag)
    true_links = extract_linear_links_for_graph(equations)

    # Create matrices for tigramite plotting
    var_names = [f'X{i+1}' for i in range(n_vars)] + ['U']
    n_total_vars = n_vars + 1
    val_matrix = np.zeros((n_total_vars, n_total_vars, max_lag + 1))
    graph_matrix = np.zeros((n_total_vars, n_total_vars, max_lag + 1), dtype='bool')

    # Fill matrices based on true links
    for (source, lag, target), weight in true_links.items():
        if source == 'U':
            source_idx = n_vars
        else:
            source_idx = int(source[1:]) - 1

        target_idx = int(target[1:]) - 1
        lag_idx = abs(lag)

        # Add the link to the matrices
        val_matrix[source_idx, target_idx, lag_idx] = weight
        graph_matrix[source_idx, target_idx, lag_idx] = True

        # For contemporaneous links, make val_matrix symmetric
        if lag == 0:
            val_matrix[target_idx, source_idx, lag_idx] = weight

    # Plot and save causal graph
    plt.figure(figsize=(12, 12))
    tp.plot_time_series_graph(
        val_matrix=val_matrix,
        graph=graph_matrix,
        var_names=var_names,
        link_colorbar_label='Nonlinear Effect Strength',
        node_size=0.05
    )
    plt.title(f'Nonlinear Causal Graph with Confounder\n(n={sample_size}, vars={n_vars}, lag={max_lag})\n{noise_type}, Block Prob {block_prob:.1f}')
    plt.savefig(f'{base_filename}_causal_graph.png')
    plt.close()

    # Plot time series with missing blocks and confounder
    plt.figure(figsize=(15, 10))

    # Plot observed variables
    for col in df_missing.columns[:-2]:  # Exclude U and time
        plt.plot(df_complete['time'], df_complete[col], label=f'{col} (complete)', alpha=0.3)
        mask = ~df_missing[col].isna()
        plt.scatter(df_missing.loc[mask, 'time'], df_missing.loc[mask, col],
                   label=f'{col} (observed)', alpha=0.7, s=20)

    # Plot confounder U (always fully observed)
    plt.plot(df_missing['time'], df_missing['U'], label='U (confounder)',
             linestyle='--', alpha=0.7, color='black')

    plt.title(f'Nonlinear Time Series with Block Missingness and Confounder\n(n={sample_size}, vars={n_vars}, lag={max_lag})')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{base_filename}_series.png')
    plt.close()

    # Plot missing data pattern
    plt.figure(figsize=(15, 5))
    plt.imshow(missing_mask.T, aspect='auto', cmap='binary')
    plt.title('Missing Data Pattern (black = missing)')
    plt.xlabel('Time')
    plt.ylabel('Variable')
    plt.yticks(range(n_vars), [f'X{i+1}' for i in range(n_vars)])
    plt.colorbar(label='Missing')
    plt.savefig(f'{base_filename}_missing_pattern.png')
    plt.close()

    # Save missing data statistics
    with open(f'{base_filename}_statistics.txt', 'w') as f:
        f.write(f"Missing Data Statistics:\n")
        f.write(f"Total points: {missing_mask.size}\n")
        f.write(f"Missing points: {missing_mask.sum()}\n")
        f.write(f"Missing percentage: {100 * missing_mask.sum() / missing_mask.size:.2f}%\n")
        f.write("\nMissing percentage by variable:\n")
        for i in range(n_vars):
            pct = 100 * missing_mask[:, i].sum() / len(missing_mask)
            f.write(f"X{i+1}: {pct:.2f}%\n")
        f.write("\nNote: Confounder U is always fully observed\n")

    # Save causal structure description
    with open(f'{base_filename}_causal_structure.txt', 'w') as f:
        f.write(f"True Nonlinear Causal Structure with Confounder:\n")
        f.write("Format: (source, lag, target) => coefficient\n")
        f.write("\nEquations:\n")
        for eq in equations:
            f.write(f"{eq}\n")
        f.write("\nNonlinear Causal Links:\n")
        for (source, lag, target), coef in true_links.items():
            f.write(f"({source}, {lag}, {target}) => {coef} [base coefficient]\n")

def analyze_block_missing_confounded_data(df_missing, df_complete, missing_mask,
                                        title="Nonlinear Time Series with Block Missingness and Confounder"):
    """Analyze and visualize block missing data patterns with confounder"""
    print(f"\nAnalyzing {title}")
    print("=" * 50)

    # Basic missing data statistics
    n_total = missing_mask.size
    n_missing = missing_mask.sum()
    missing_rate = n_missing / n_total

    print(f"\nMissing Data Summary:")
    print(f"Total values: {n_total}")
    print(f"Missing values: {n_missing}")
    print(f"Overall missing rate: {missing_rate:.2%}")
    print("Note: Confounder U is always fully observed")

    # Analyze block characteristics
    block_lengths = []
    current_block = 0

    for col in range(missing_mask.shape[1]):
        block_start = False
        for row in range(missing_mask.shape[0]):
            if missing_mask[row, col]:
                if not block_start:
                    block_start = True
                    current_block = 1
                else:
                    current_block += 1
            elif block_start:
                block_lengths.append(current_block)
                block_start = False
                current_block = 0
        if block_start:  # Handle case where block ends at the end of series
            block_lengths.append(current_block)

    if block_lengths:
        print("\nBlock Length Statistics:")
        print(f"Number of blocks: {len(block_lengths)}")
        print(f"Mean block length: {np.mean(block_lengths):.2f}")
        print(f"Median block length: {np.median(block_lengths):.2f}")
        print(f"Min block length: {min(block_lengths)}")
        print(f"Max block length: {max(block_lengths)}")

    # Statistical comparison between complete and observed data
    print("\nStatistical Comparison (Complete vs. Observed):")
    for col in df_missing.columns[:-2]:  # Exclude U and time
        complete_stats = df_complete[col]
        observed_stats = df_missing[col].dropna()

        print(f"\n{col}:")
        print(f"Complete  - Mean: {np.mean(complete_stats):.3f}, Std: {np.std(complete_stats):.3f}")
        print(f"Observed - Mean: {np.mean(observed_stats):.3f}, Std: {np.std(observed_stats):.3f}")

    # Confounder analysis
    print("\nConfounder (U) Analysis:")
    print(f"U - Mean: {np.mean(df_missing['U']):.3f}, Std: {np.std(df_missing['U']):.3f}")
    print("\nCorrelations with U:")
    correlations = df_missing.drop('time', axis=1).corr()['U'].sort_values(ascending=False)
    print(correlations)

    return block_lengths

def generate_all_combinations():
    """Generate datasets for all combinations of parameters"""
    sample_sizes = [500, 1000, 3000, 5000]
    n_vars_list = [4, 6, 8]
    max_lags = [2, 3, 4]
    noise_types = ['gaussian', 'student_t']
    block_probs = [0.1, 0.2, 0.3]

    for n in sample_sizes:
        for vars in n_vars_list:
            for lag in max_lags:
                for noise_type in noise_types:
                    for prob in block_probs:
                        print(f"\nGenerating dataset: n={n}, vars={vars}, lag={lag}, "
                              f"noise={noise_type}, block_prob={prob:.1f}")

                        # Configure generator
                        noise_params = {'scale': 0.1, 'df': 3} if noise_type == 'student_t' else {'scale': 0.1}
                        generator = BlockMissingNonlinearConfoundedGenerator(
                            block_size_range=(5, 20),
                            block_prob=prob,
                            noise_type=noise_type,
                            noise_params=noise_params,
                            random_state=42
                        )

                        # Generate dataset
                        df_missing, df_complete, missing_mask = generator.generate_multivariate_ts(
                            n_points=n,
                            n_vars=vars,
                            max_lag=lag,
                            total_time=100,
                            min_gap=0.1
                        )

                        # Save dataset and create visualizations
                        save_dataset_and_visualizations(
                            df_missing=df_missing,
                            df_complete=df_complete,
                            missing_mask=missing_mask,
                            n_vars=vars,
                            max_lag=lag,
                            sample_size=n,
                            noise_type=noise_type,
                            block_prob=prob
                        )
                        print("Dataset and visualizations saved successfully")

if __name__ == "__main__":
    print("Generating nonlinear time series with block missingness and confounder...")

    # Example case for testing
    # n_points = 1000
    # n_vars = 4
    # max_lag = 2
    # block_prob = 0.2

    # # Generate example with Gaussian noise
    # generator = BlockMissingNonlinearConfoundedGenerator(
    #     block_size_range=(5, 20),
    #     block_prob=block_prob,
    #     noise_type='gaussian',
    #     noise_params={'scale': 0.1},
    #     random_state=42
    # )

    # # Generate dataset
    # df_missing, df_complete, missing_mask = generator.generate_multivariate_ts(
    #     n_points=n_points,
    #     n_vars=n_vars,
    #     max_lag=max_lag,
    #     total_time=100,
    #     min_gap=0.1
    # )

    # # Save and analyze example case
    # save_dataset_and_visualizations(
    #     df_missing=df_missing,
    #     df_complete=df_complete,
    #     missing_mask=missing_mask,
    #     n_vars=n_vars,
    #     max_lag=max_lag,
    #     sample_size=n_points,
    #     noise_type='gaussian',
    #     block_prob=block_prob
    # )

    # # Analyze the data
    # block_lengths = analyze_block_missing_confounded_data(
    #     df_missing,
    #     df_complete,
    #     missing_mask,
    #     "Gaussian Example with Block Missingness and Confounder"
    # )

    # Generate all combinations
    generate_all_combinations()

!zip -r /content/output_D2C.zip /content/output_block_missing_confounded
from google.colab import files
files.download('/content/output_D2C.zip')