{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHX6whEvfwNd",
        "outputId": "4218a280-af6c-494b-8ec4-a09da3100aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tigramite\n",
            "  Downloading tigramite-5.2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from tigramite) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tigramite) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tigramite) (1.17.0)\n",
            "Downloading tigramite-5.2.7.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.6/309.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tigramite\n",
            "Successfully installed tigramite-5.2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tigramite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tigramite import plotting as tp\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "I0OhylZrgutF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Nonlinear Equations (Ensuring U has only 2 edges)\n",
        "\n",
        "def get_nonlinear_equations(n_vars, max_lag):\n",
        "    \"\"\"\n",
        "    Return a list of equation strings describing the structural equations\n",
        "    for each X_i[t], plus the confounder U[t]. We ensure that U has only 2 edges.\n",
        "    \"\"\"\n",
        "    if n_vars == 4:\n",
        "        if max_lag == 2:\n",
        "            return [\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + 0.3 * U[t] + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 3:\n",
        "            return [\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + 0.3 * U[t] + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 4:\n",
        "            return [\n",
        "                \"X4[t] = 0.25 * cos(X1[t-4] * pi/2) + 0.3 * U[t] + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "\n",
        "    elif n_vars == 6:\n",
        "        # Keep U->X6, U->X1\n",
        "        if max_lag == 2:\n",
        "            return [\n",
        "                \"X6[t] = 0.85 * sin(X5[t] * pi/2) + 0.4 * U[t] + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + 0.3 * sin(X5[t-1] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 3:\n",
        "            return [\n",
        "                \"X6[t] = 0.85 * sin(X5[t] * pi/2) + 0.4 * U[t] + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 4:\n",
        "            return [\n",
        "                \"X6[t] = 0.85 * sin(X5[t] * pi/2) + 0.4 * U[t] + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-4] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "\n",
        "    elif n_vars == 8:\n",
        "        # Keep U->X8, U->X1\n",
        "        if max_lag == 2:\n",
        "            return [\n",
        "                \"X8[t] = 0.4 * sin(X7[t] * pi/2) + 0.35 * U[t] + trend8[t] + season8[t] + e8\",\n",
        "                \"X7[t] = 0.35 * cos(X6[t-1] * pi/2) + trend7[t] + season7[t] + e7\",\n",
        "                \"X6[t] = 0.45 * sin(X5[t] * pi/2) + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + 0.3 * sin(X5[t-1] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 3:\n",
        "            return [\n",
        "                \"X8[t] = 0.4 * sin(X7[t] * pi/2) + 0.35 * U[t] + trend8[t] + season8[t] + e8\",\n",
        "                \"X7[t] = 0.35 * cos(X6[t-1] * pi/2) + trend7[t] + season7[t] + e7\",\n",
        "                \"X6[t] = 0.45 * sin(X5[t] * pi/2) + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-2] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "        elif max_lag == 4:\n",
        "            return [\n",
        "                \"X8[t] = 0.4 * sin(X7[t] * pi/2) + 0.35 * U[t] + trend8[t] + season8[t] + e8\",\n",
        "                \"X7[t] = 0.35 * cos(X6[t-1] * pi/2) + trend7[t] + season7[t] + e7\",\n",
        "                \"X6[t] = 0.45 * sin(X5[t] * pi/2) + trend6[t] + season6[t] + e6\",\n",
        "                \"X5[t] = 0.4 * cos(X4[t-1] * pi/2) + trend5[t] + season5[t] + e5\",\n",
        "                \"X4[t] = 0.25 * cos(X1[t-4] * pi/2) + trend4[t] + season4[t] + e4\",\n",
        "                \"X3[t] = 0.35 * (X4[t])^2 + 0.2 * cos(X2[t-3] * pi/2) + trend3[t] + season3[t] + e3\",\n",
        "                \"X2[t] = 0.3 * sin(X3[t-1] * pi/2) + trend2[t] + season2[t] + e2\",\n",
        "                \"X1[t] = 0.4 * (X2[t])^3 + 0.5 * U[t] + trend1[t] + season1[t] + e1\",\n",
        "                \"U[t] = eU\"\n",
        "            ]\n",
        "\n",
        "    # If no match, return empty\n",
        "    return []\n",
        "\n",
        "\n",
        "# Nonlinear Time Series Generator Class\n",
        "\n",
        "class NonlinearTimeSeriesGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_scale=0.1,\n",
        "        trend_strength=0.01,\n",
        "        seasonal_strength=0.5,\n",
        "        seasonal_period=12,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.noise_scale = noise_scale\n",
        "        self.trend_strength = trend_strength\n",
        "        self.seasonal_strength = seasonal_strength\n",
        "        self.seasonal_period = seasonal_period\n",
        "        self.random_state = random_state\n",
        "        if random_state is not None:\n",
        "            np.random.seed(random_state)\n",
        "\n",
        "    def generate_noise(self, size):\n",
        "        return np.random.normal(0, self.noise_scale, size=size)\n",
        "\n",
        "    def generate_trend(self, n_points, var_idx):\n",
        "        trend_modifier = (var_idx + 1) * 0.5\n",
        "        t = np.arange(n_points)\n",
        "        return self.trend_strength * trend_modifier * t\n",
        "\n",
        "    def generate_seasonality(self, n_points, var_idx):\n",
        "        phase_shift = 2 * np.pi * var_idx / 8\n",
        "        t = np.arange(n_points)\n",
        "        season1 = np.sin(2 * np.pi * t / self.seasonal_period + phase_shift)\n",
        "        season2 = 0.5 * np.cos(4 * np.pi * t / self.seasonal_period + phase_shift)\n",
        "        return self.seasonal_strength * (season1 + season2)\n",
        "\n",
        "    def evaluate_term(self, term, var_values, X, U, t):\n",
        "        \"\"\"Safely parse and evaluate one term, e.g. '0.3 * sin(X2[t-1] * pi/2)'.\"\"\"\n",
        "        parts = term.split('*')\n",
        "        coef = float(parts[0].strip())\n",
        "        expr = parts[1].strip()\n",
        "\n",
        "        # Helper to parse lag safely\n",
        "        def parse_lag(lag_str):\n",
        "            if 't-' in lag_str:\n",
        "                # e.g., \"t-2\"\n",
        "                return int(lag_str.split('-')[1])\n",
        "            # if there's no '-', default to 0\n",
        "            return 0\n",
        "\n",
        "        # 1) Check for confounder U\n",
        "        if expr.startswith('U['):\n",
        "            # e.g. \"U[t]\" or \"U[t-1]\"\n",
        "            if expr == 'U[t]':\n",
        "                return coef * U[t]\n",
        "            else:\n",
        "                # e.g. \"U[t-2]\"\n",
        "                lagval = parse_lag(expr.split('[')[1].split(']')[0])\n",
        "                if t - lagval >= 0:\n",
        "                    return coef * U[t - lagval]\n",
        "                else:\n",
        "                    return 0.0\n",
        "\n",
        "        # 2) If it has [t- but not cos/sin/^ => direct AR (X4[t-2])\n",
        "        if '[t-' in expr and not any(func in expr for func in ['cos', 'sin', '^']):\n",
        "            var_index = int(expr.split('X')[1].split('[')[0]) - 1\n",
        "            lagval = parse_lag(expr.split('[')[1].split(']')[0])\n",
        "            if t - lagval >= 0:\n",
        "                return coef * X[t - lagval, var_index]\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        # 3) Nonlinear terms\n",
        "        if 'cos(' in expr:\n",
        "            # e.g. \"cos(X1[t-2] * pi/2)\"\n",
        "            inner = expr.split('cos(')[1].split(')')[0]\n",
        "            var_idx_ = int(inner.split('X')[1].split('[')[0]) - 1\n",
        "            lag_str = inner.split('[')[1].split(']')[0]\n",
        "            lagval = parse_lag(lag_str)\n",
        "            val = X[t - lagval, var_idx_] if t - lagval >= 0 else 0.0\n",
        "            return coef * np.cos(val * np.pi / 2)\n",
        "\n",
        "        elif 'sin(' in expr:\n",
        "            # e.g. \"sin(X3[t-1] * pi/2)\"\n",
        "            inner = expr.split('sin(')[1].split(')')[0]\n",
        "            var_idx_ = int(inner.split('X')[1].split('[')[0]) - 1\n",
        "            lag_str = inner.split('[')[1].split(']')[0]\n",
        "            lagval = parse_lag(lag_str)\n",
        "            val = X[t - lagval, var_idx_] if t - lagval >= 0 else 0.0\n",
        "            return coef * np.sin(val * np.pi / 2)\n",
        "\n",
        "        elif '^' in expr:\n",
        "            # e.g. \"(X2[t])^3\"\n",
        "            power = int(expr.split('^')[1])\n",
        "            var_idx_ = int(expr.split('X')[1].split('[')[0]) - 1\n",
        "            if '[t]' in expr:\n",
        "                val = X[t, var_idx_]\n",
        "            else:\n",
        "                lagval = parse_lag(expr.split('-')[1].split(']')[0]) if '[t-' in expr else 0\n",
        "                val = X[t - lagval, var_idx_] if t - lagval >= 0 else 0.0\n",
        "            return coef * (val ** power)\n",
        "\n",
        "        # 4) Default linear e.g. \"X2[t]\"\n",
        "        var_idx_ = int(expr.split('X')[1].split('[')[0]) - 1\n",
        "        if '[t-' in expr:\n",
        "            lagval = parse_lag(expr.split('[')[1].split(']')[0])\n",
        "            val = X[t - lagval, var_idx_] if t - lagval >= 0 else 0.0\n",
        "        else:\n",
        "            # e.g. \"X2[t]\"\n",
        "            val = X[t, var_idx_]\n",
        "        return coef * val\n",
        "\n",
        "    def generate_equations(self, t, X, U, trends, seasonality, n_vars, max_lag):\n",
        "        noise = self.generate_noise(n_vars + 1)\n",
        "        equations = get_nonlinear_equations(n_vars, max_lag)\n",
        "\n",
        "        # Confounder first\n",
        "        U[t] = noise[-1]\n",
        "        var_values = {'U': U[t]}\n",
        "\n",
        "        for eq in equations:\n",
        "            if '=' not in eq or eq.startswith('U['):\n",
        "                continue\n",
        "\n",
        "            left, right = eq.split('=')\n",
        "            left = left.strip()\n",
        "            right = right.strip()\n",
        "\n",
        "            var_name = left.split('[')[0]\n",
        "            var_idx = int(var_name[1:]) - 1\n",
        "\n",
        "            terms = right.split('+')\n",
        "            val = 0.0\n",
        "\n",
        "            for term in terms:\n",
        "                term = term.strip()\n",
        "                if term.startswith('e'):\n",
        "                    val += noise[var_idx]\n",
        "                elif term.startswith('trend'):\n",
        "                    val += trends[var_idx][t]\n",
        "                elif term.startswith('season'):\n",
        "                    val += seasonality[var_idx][t]\n",
        "                else:\n",
        "                    # Evaluate e.g. \"0.3 * sin(X3[t-1]*pi/2)\"\n",
        "                    val += self.evaluate_term(term, var_values, X, U, t)\n",
        "\n",
        "            X[t, var_idx] = val\n",
        "            var_values[var_name] = val\n",
        "\n",
        "    def generate_multivariate_ts(self, n_points, n_vars, max_lag):\n",
        "        X = np.zeros((n_points, n_vars))\n",
        "        U = np.zeros(n_points)\n",
        "\n",
        "        trends = [self.generate_trend(n_points, i) for i in range(n_vars)]\n",
        "        seasonality = [self.generate_seasonality(n_points, i) for i in range(n_vars)]\n",
        "\n",
        "        # Initialize first 'max_lag' steps\n",
        "        for i in range(max_lag):\n",
        "            X[i, :] = self.generate_noise(n_vars)\n",
        "            U[i] = self.generate_noise(1)[0]\n",
        "            for j in range(n_vars):\n",
        "                X[i, j] += trends[j][i] + seasonality[j][i]\n",
        "\n",
        "        # Then fill from t=max_lag onward\n",
        "        for t in range(max_lag, n_points):\n",
        "            self.generate_equations(t, X, U, trends, seasonality, n_vars, max_lag)\n",
        "\n",
        "        df = pd.DataFrame(X, columns=[f\"X{i+1}\" for i in range(n_vars)])\n",
        "        df[\"U\"] = U\n",
        "        df[\"time\"] = np.arange(n_points)\n",
        "        return df\n",
        "\n",
        "\n",
        "# Helper Functions for Graph & Saving\n",
        "\n",
        "def extract_causal_links(equations):\n",
        "    links = {}\n",
        "    for eq in equations:\n",
        "        if '=' not in eq or eq.startswith('U['):\n",
        "            continue\n",
        "        left, right = eq.split('=')\n",
        "        left = left.strip()\n",
        "        right = right.strip()\n",
        "        target = f\"X{int(left.split('X')[1].split('[')[0])}\"\n",
        "\n",
        "        for term in right.split('+'):\n",
        "            term = term.strip()\n",
        "            if 'U' in term:\n",
        "                coef = float(term.split('*')[0])\n",
        "                links[('U', 0, target, 'confounder')] = coef\n",
        "                continue\n",
        "\n",
        "            if 'X' not in term:\n",
        "                continue\n",
        "\n",
        "            coef = float(term.split('*')[0])\n",
        "            if 'cos(' in term:\n",
        "                func = 'cos'\n",
        "                source_var = int(term.split('X')[1].split('[')[0])\n",
        "                source = f\"X{source_var}\"\n",
        "            elif 'sin(' in term:\n",
        "                func = 'sin'\n",
        "                source_var = int(term.split('X')[1].split('[')[0])\n",
        "                source = f\"X{source_var}\"\n",
        "            elif '^' in term:\n",
        "                func = f\"power{term.split('^')[1]}\"\n",
        "                source_var = int(term.split('X')[1].split('[')[0])\n",
        "                source = f\"X{source_var}\"\n",
        "            else:\n",
        "                func = 'linear'\n",
        "                source_var = int(term.split('X')[1].split('[')[0])\n",
        "                source = f\"X{source_var}\"\n",
        "\n",
        "            if '[t-' in term:\n",
        "                lag = int(term.split('-')[1].split(']')[0])\n",
        "            else:\n",
        "                lag = 0\n",
        "\n",
        "            links[(source, lag, target, func)] = coef\n",
        "\n",
        "    return links\n",
        "\n",
        "def save_dataset_and_graph(df, n_vars, max_lag, sample_size, output_dir=\"output_nonlinear\"):\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    base_filename = f\"{output_dir}/nonlinear_ts_with_confounder_n{sample_size}_vars{n_vars}_lag{max_lag}\"\n",
        "\n",
        "    # 1) Save CSV\n",
        "    df.to_csv(f\"{base_filename}.csv\", index=False)\n",
        "\n",
        "    # 2) Build adjacency from equations\n",
        "    equations = get_nonlinear_equations(n_vars, max_lag)\n",
        "    true_links = extract_causal_links(equations)\n",
        "\n",
        "    var_names = [f\"X{i+1}\" for i in range(n_vars)] + [\"U\"]\n",
        "    n_total_vars = n_vars + 1\n",
        "    val_matrix = np.zeros((n_total_vars, n_total_vars, max_lag + 1))\n",
        "    graph_matrix = np.zeros((n_total_vars, n_total_vars, max_lag + 1), dtype=bool)\n",
        "\n",
        "    for (source, lag, target, func), weight in true_links.items():\n",
        "        if source == 'U':\n",
        "            source_idx = n_vars\n",
        "        else:\n",
        "            source_idx = int(source[1:]) - 1\n",
        "        if target == 'U':\n",
        "            target_idx = n_vars\n",
        "        else:\n",
        "            target_idx = int(target[1:]) - 1\n",
        "\n",
        "        val_matrix[source_idx, target_idx, abs(lag)] = weight\n",
        "        graph_matrix[source_idx, target_idx, abs(lag)] = True\n",
        "        if lag == 0:\n",
        "            val_matrix[target_idx, source_idx, 0] = weight\n",
        "\n",
        "    # Plot adjacency\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    tp.plot_time_series_graph(\n",
        "        val_matrix=val_matrix,\n",
        "        graph=graph_matrix,\n",
        "        var_names=var_names,\n",
        "        link_colorbar_label='Nonlinear Effect Strength',\n",
        "        node_size=0.05\n",
        "    )\n",
        "    plt.title(f'Nonlinear Causal Graph with Confounder\\n(n={sample_size}, vars={n_vars}, lag={max_lag})')\n",
        "    plt.savefig(f'{base_filename}_graph.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot time series\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for col in df.columns[:-2]:  # exclude U and time\n",
        "        plt.plot(df['time'], df[col], label=col, alpha=0.7)\n",
        "    plt.title(f'Nonlinear Time Series with Confounder\\n(n={sample_size}, vars={n_vars}, lag={max_lag})')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{base_filename}_series.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Decomposition (simple trend overlay)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    n_cols = min(3, n_vars)\n",
        "    n_rows = (n_vars + n_cols - 1) // n_cols\n",
        "    for i, col in enumerate(df.columns[:-2]):\n",
        "        plt.subplot(n_rows, n_cols, i+1)\n",
        "        plt.plot(df['time'], df[col], label='Series', alpha=0.7)\n",
        "        z = np.polyfit(df['time'], df[col], 1)\n",
        "        p = np.poly1d(z)\n",
        "        plt.plot(df['time'], p(df['time']), \"r--\", label='Trend')\n",
        "        plt.title(f'{col} Components')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{base_filename}_decomposition.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save structure\n",
        "    with open(f'{base_filename}_structure.txt', 'w') as f:\n",
        "        f.write(\"Nonlinear Causal Structure with Confounder:\\n\\n\")\n",
        "        f.write(\"Equations:\\n\")\n",
        "        for eq in equations:\n",
        "            f.write(f\"{eq}\\n\")\n",
        "        f.write(\"\\nCausal Links:\\n\")\n",
        "        for (source, lag, target, func), coef in true_links.items():\n",
        "            f.write(f\"{source} --({func}, lag={lag})--> {target}: {coef}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Generate Multiple Combinations\n",
        "\n",
        "def generate_all_combinations():\n",
        "    sample_sizes = [500, 1000, 3000]\n",
        "    n_vars_list = [4, 6, 8]\n",
        "    max_lags = [2, 3, 4]\n",
        "    trend_strengths = [0.01, 0.02]\n",
        "    seasonal_strengths = [0.3, 0.5]\n",
        "\n",
        "    for n in sample_sizes:\n",
        "        for vars_ in n_vars_list:\n",
        "            for lag in max_lags:\n",
        "                for trend_str in trend_strengths:\n",
        "                    for seas_str in seasonal_strengths:\n",
        "                        print(f\"\\nGenerating dataset: n={n}, vars={vars_}, lag={lag}, trend={trend_str}, seas={seas_str}\")\n",
        "                        generator = NonlinearTimeSeriesGenerator(\n",
        "                            noise_scale=0.1,\n",
        "                            trend_strength=trend_str,\n",
        "                            seasonal_strength=seas_str,\n",
        "                            seasonal_period=12,\n",
        "                            random_state=42\n",
        "                        )\n",
        "                        df = generator.generate_multivariate_ts(\n",
        "                            n_points=n,\n",
        "                            n_vars=vars_,\n",
        "                            max_lag=lag\n",
        "                        )\n",
        "                        save_dataset_and_graph(df, vars_, lag, n)\n",
        "                        print(\" -> Saved successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "#  Main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Generating nonlinear time series with safe lag parsing...\")\n",
        "\n",
        "    # Example usage\n",
        "    sample_sizes = [500, 1000, 3000, 5000]\n",
        "    n_vars_list = [4, 6, 8]\n",
        "    max_lags = [2, 3, 4]\n",
        "\n",
        "    for n in sample_sizes:\n",
        "        for vars_ in n_vars_list:\n",
        "            for lag in max_lags:\n",
        "                print(f\"\\nGenerating dataset: n={n}, vars={vars_}, lag={lag}\")\n",
        "                gen = NonlinearTimeSeriesGenerator(\n",
        "                    noise_scale=0.1,\n",
        "                    trend_strength=0.01,\n",
        "                    seasonal_strength=0.5,\n",
        "                    seasonal_period=12,\n",
        "                    random_state=42\n",
        "                )\n",
        "                df = gen.generate_multivariate_ts(\n",
        "                    n_points=n,\n",
        "                    n_vars=vars_,\n",
        "                    max_lag=lag\n",
        "                )\n",
        "                save_dataset_and_graph(df, vars_, lag, n)\n",
        "                print(f\"Dataset and plots saved for n_vars={vars_}, lag={lag}.\")\n",
        "\n",
        "    print(\"\\nAll done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z-1fZoyoqGgd",
        "outputId": "fd6e6dd5-ff07-4ab0-b64d-9388ce40142f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating nonlinear time series with safe lag parsing...\n",
            "\n",
            "Generating dataset: n=500, vars=4, lag=2\n",
            "Dataset and plots saved for n_vars=4, lag=2.\n",
            "\n",
            "Generating dataset: n=500, vars=4, lag=3\n",
            "Dataset and plots saved for n_vars=4, lag=3.\n",
            "\n",
            "Generating dataset: n=500, vars=4, lag=4\n",
            "Dataset and plots saved for n_vars=4, lag=4.\n",
            "\n",
            "Generating dataset: n=500, vars=6, lag=2\n",
            "Dataset and plots saved for n_vars=6, lag=2.\n",
            "\n",
            "Generating dataset: n=500, vars=6, lag=3\n",
            "Dataset and plots saved for n_vars=6, lag=3.\n",
            "\n",
            "Generating dataset: n=500, vars=6, lag=4\n",
            "Dataset and plots saved for n_vars=6, lag=4.\n",
            "\n",
            "Generating dataset: n=500, vars=8, lag=2\n",
            "Dataset and plots saved for n_vars=8, lag=2.\n",
            "\n",
            "Generating dataset: n=500, vars=8, lag=3\n",
            "Dataset and plots saved for n_vars=8, lag=3.\n",
            "\n",
            "Generating dataset: n=500, vars=8, lag=4\n",
            "Dataset and plots saved for n_vars=8, lag=4.\n",
            "\n",
            "Generating dataset: n=1000, vars=4, lag=2\n",
            "Dataset and plots saved for n_vars=4, lag=2.\n",
            "\n",
            "Generating dataset: n=1000, vars=4, lag=3\n",
            "Dataset and plots saved for n_vars=4, lag=3.\n",
            "\n",
            "Generating dataset: n=1000, vars=4, lag=4\n",
            "Dataset and plots saved for n_vars=4, lag=4.\n",
            "\n",
            "Generating dataset: n=1000, vars=6, lag=2\n",
            "Dataset and plots saved for n_vars=6, lag=2.\n",
            "\n",
            "Generating dataset: n=1000, vars=6, lag=3\n",
            "Dataset and plots saved for n_vars=6, lag=3.\n",
            "\n",
            "Generating dataset: n=1000, vars=6, lag=4\n",
            "Dataset and plots saved for n_vars=6, lag=4.\n",
            "\n",
            "Generating dataset: n=1000, vars=8, lag=2\n",
            "Dataset and plots saved for n_vars=8, lag=2.\n",
            "\n",
            "Generating dataset: n=1000, vars=8, lag=3\n",
            "Dataset and plots saved for n_vars=8, lag=3.\n",
            "\n",
            "Generating dataset: n=1000, vars=8, lag=4\n",
            "Dataset and plots saved for n_vars=8, lag=4.\n",
            "\n",
            "Generating dataset: n=3000, vars=4, lag=2\n",
            "Dataset and plots saved for n_vars=4, lag=2.\n",
            "\n",
            "Generating dataset: n=3000, vars=4, lag=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tigramite/plotting.py:3203: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig = pyplot.figure(figsize=figsize)\n",
            "<ipython-input-13-11f8f39240fe>:376: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(15, 10))\n",
            "<ipython-input-13-11f8f39240fe>:388: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(15, 15))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset and plots saved for n_vars=4, lag=3.\n",
            "\n",
            "Generating dataset: n=3000, vars=4, lag=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-11f8f39240fe>:363: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(12, 12))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset and plots saved for n_vars=4, lag=4.\n",
            "\n",
            "Generating dataset: n=3000, vars=6, lag=2\n",
            "Dataset and plots saved for n_vars=6, lag=2.\n",
            "\n",
            "Generating dataset: n=3000, vars=6, lag=3\n",
            "Dataset and plots saved for n_vars=6, lag=3.\n",
            "\n",
            "Generating dataset: n=3000, vars=6, lag=4\n",
            "Dataset and plots saved for n_vars=6, lag=4.\n",
            "\n",
            "Generating dataset: n=3000, vars=8, lag=2\n",
            "Dataset and plots saved for n_vars=8, lag=2.\n",
            "\n",
            "Generating dataset: n=3000, vars=8, lag=3\n",
            "Dataset and plots saved for n_vars=8, lag=3.\n",
            "\n",
            "Generating dataset: n=3000, vars=8, lag=4\n",
            "Dataset and plots saved for n_vars=8, lag=4.\n",
            "\n",
            "Generating dataset: n=5000, vars=4, lag=2\n",
            "Dataset and plots saved for n_vars=4, lag=2.\n",
            "\n",
            "Generating dataset: n=5000, vars=4, lag=3\n",
            "Dataset and plots saved for n_vars=4, lag=3.\n",
            "\n",
            "Generating dataset: n=5000, vars=4, lag=4\n",
            "Dataset and plots saved for n_vars=4, lag=4.\n",
            "\n",
            "Generating dataset: n=5000, vars=6, lag=2\n",
            "Dataset and plots saved for n_vars=6, lag=2.\n",
            "\n",
            "Generating dataset: n=5000, vars=6, lag=3\n",
            "Dataset and plots saved for n_vars=6, lag=3.\n",
            "\n",
            "Generating dataset: n=5000, vars=6, lag=4\n",
            "Dataset and plots saved for n_vars=6, lag=4.\n",
            "\n",
            "Generating dataset: n=5000, vars=8, lag=2\n",
            "Dataset and plots saved for n_vars=8, lag=2.\n",
            "\n",
            "Generating dataset: n=5000, vars=8, lag=3\n",
            "Dataset and plots saved for n_vars=8, lag=3.\n",
            "\n",
            "Generating dataset: n=5000, vars=8, lag=4\n",
            "Dataset and plots saved for n_vars=8, lag=4.\n",
            "\n",
            "All done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Zip and download output\n",
        "!zip -r /content/output_nonlinear_with_confounder.zip /content/output_nonlinear\n",
        "from google.colab import files\n",
        "files.download('/content/output_nonlinear_with_confounder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S2hBapXggd-F",
        "outputId": "7ba7ddea-f421-40b9-b6d5-415f9b400533"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output_nonlinear/ (stored 0%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag3_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag2.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag4.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag4_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag2_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag2_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag2_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag3_series.png (deflated 10%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag3_series.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag3_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag4.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag2_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag2.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag4_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag3_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag3_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag4_series.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag2_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag2_structure.txt (deflated 63%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag4.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag4_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag4_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag3_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag2_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag4_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag2_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag4_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag4_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag3_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag4.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag4_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag3.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag3.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag2_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag4_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag4.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag2_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag4.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag3_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag3.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag3.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag2_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag4_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag3_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag2_structure.txt (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag2.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag2_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag4_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag4_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag2_structure.txt (deflated 63%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag4.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag2_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag2_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag4_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag3_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag2_structure.txt (deflated 63%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag4_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag2_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag4_series.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag3_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag2_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag3_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag4_decomposition.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag3.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag2_decomposition.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag3_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag4_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag4.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag3.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag3_decomposition.png (deflated 3%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag3_decomposition.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag2_structure.txt (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag3.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag3.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag3_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag4_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag2_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag2.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag3_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag2_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag3_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag3_series.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag2.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag4_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag3_decomposition.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag3_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag4_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag3_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag2_series.png (deflated 7%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag2_decomposition.png (deflated 3%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag2_structure.txt (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag2_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag4.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag4_series.png (deflated 10%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag2_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag2_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag3_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag2.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag3.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag4_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag4_decomposition.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag3_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag3_decomposition.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag4_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag4_decomposition.png (deflated 3%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag4_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag2_structure.txt (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag3_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag3_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag4_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag4_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag3_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag2_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag2_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag4.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag3_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag3_decomposition.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag4_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag2_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag3_series.png (deflated 9%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag2.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag2.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag4.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag2.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag2_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag3_structure.txt (deflated 62%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag4_series.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag4_decomposition.png (deflated 8%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag3.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag2_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag2_structure.txt (deflated 63%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag3_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag2_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars6_lag2.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag2.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag3.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars6_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag4_series.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag4_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars8_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars6_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag4_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag2_decomposition.png (deflated 6%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag3_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag4_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars6_lag4_structure.txt (deflated 59%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag2_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars4_lag4.csv (deflated 52%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars8_lag3.csv (deflated 51%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag3_structure.txt (deflated 53%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag2.csv (deflated 50%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars4_lag2_graph.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n1000_vars8_lag2_decomposition.png (deflated 5%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n500_vars4_lag3_graph.png (deflated 4%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n5000_vars8_lag3_series.png (deflated 13%)\n",
            "  adding: content/output_nonlinear/nonlinear_ts_with_confounder_n3000_vars4_lag4_graph.png (deflated 4%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8c1cc79c-9878-4003-b489-a39c45528708\", \"output_nonlinear_with_confounder.zip\", 19231603)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}